---
title: "Kapitel 3: Stikprøvegennemsnit og Stikprøvekovarianser - Øvelser"
author: "Kasper Veje Jakobsen"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: false
    theme: flatly
    highlight: tango
    df_print: paged
    code_folding: show
---

```{r}
# Indlæsning af libraries
library(st514)
library(rgl)
library(car)
library(sn)

# Ryd op
rm(list=ls())
```


# Opgave 3.1

Vi bruger følgende matrix til at besvare opgaverne:

$$
X = \begin{bmatrix}
9 & 1 \\
5 & 3 \\
1 & 2
\end{bmatrix}
$$

## Opgave a

Tegn scatterplottet i p = 2 dimensioner. Placér stikprøvegennemsnittet på dit diagram.

```{r, warning=FALSE}
# Definering af matrix X
X <- matrix(c(9, 5, 1, 1, 3, 2), 3, 2)

# Plot af data
plot(X, xlim = c(0, 10), ylim = c(0, 4), 
     main = "Scatterplot med stikprøvegennemsnit",
     xlab = "X1", ylab = "X2", pch = 16)

# Beregning af stikprøvegennemsnit
x_bar <- colMeans(X)

# Tilføj stikprøvegennemsnit til plot
points(x_bar[1], x_bar[2], pch = "X", col = "red", cex = 2)

# Tilføj en forklaring
legend("topleft", legend = c("Observation", "Stikprøvegennemsnit"), 
       pch = c(16, "X"), col = c("black", "red"))

# Vis stikprøvegennemsnittet
cat("Stikprøvegennemsnittet er:", x_bar, "\n")
```

## Opgave c

Tegn afvigelsevektorerne udgående fra origo. Beregn længderne af disse vektorer og cosinus til vinklen mellem dem. Relatér disse størrelser til $S_n$ og $R$.

```{r}
# Definering af en vektor af 1-taller
Ones <- rep(1, 3)

# Beregn afvigelsesmatrix
Y <- X - Ones %*% t(x_bar)

# Udtræk kolonnerne som vektorer
Y1 <- Y[,1]
Y2 <- Y[,2]

# Plot afvigelsesvektorer fra origo (i 3D - kan kun ses hvis man kører det i Rstudio)
# Kræver installation af Xquartz hvis man har Mac
plot3d(t(X)[,1], t(X)[,2], t(X)[,3], xlim = c(-5, 9), ylim = c(-5, 9), zlim = c(-5, 9))
plot3d(c(Y1[1], 0), c(Y1[2], 0), c(Y1[3], 0), add = T, type = "l")
plot3d(c(Y2[1], 0), c(Y2[2], 0), c(Y2[3], 0), add = T, type = "l")

# Beregn længden af vektorerne
Ly1 <- sqrt(sum(Y1^2))
Ly2 <- sqrt(sum(Y2^2))

# Beregn cosinus til vinklen mellem vektorerne
costh <- sum(Y1 * Y2) / (Ly1 * Ly2)
ang <- acos(costh) * 180 / pi

# Beregn stikprøvevarianser og kovarianser
s11 <- sum(Y1^2) / nrow(X)
s22 <- sum(Y2^2) / nrow(X)
s12 <- sum(Y1 * Y2) / nrow(X)
r12 <- s12 / sqrt(s11 * s22)

# Vis resultater
cat("Længde af afvigelsesvektor Y1:", Ly1, "\n",
    "Længde af afvigelsesvektor Y2:", Ly2, "\n",
    "Cosinus til vinklen mellem vektorerne:", costh, "\n",
    "Vinkel mellem vektorerne (grader):", ang, "\n",
    "Varians for X1 (s11):", s11, "\n",
    "Varians for X2 (s22):", s22, "\n",
    "Kovarians mellem X1 og X2 (s12):", s12, "\n",
    "Korrelationskoefficient (r12):", r12, "\n")
```

Længderne af afvigelsesvektorerne er relateret til stikprøvevarianserne $s_{11}$ og $s_{22}$ gennem formlen $s_{ii} = \frac{1}{n} \sum_{j=1}^{n} (x_{ji} - \bar{x}_i)^2 = \frac{1}{n} \|Y_i\|^2$. Cosinus til vinklen mellem afvigelsesvektorerne er lig med korrelationskoefficienten $r_{12}$.

# Opgave 3.3

Udfør dekomponering af $y_1$ i $x_1\mathbf{1}$ og $y_1 - x_1\mathbf{1}$ ved brug af første kolonne af matricen i Eksempel 3.9.

```{r}
# Definering af matrix X fra Eksempel 3.9
X <- matrix(c(1, 4, 4, 2, 1, 0, 5, 6, 4), 3, 3)

# Udtræk første kolonne
y1 <- X[,1]

# Beregn gennemsnittet af første kolonne
x1_bar <- mean(y1)

# Definering af en vektor af 1-taller
Ones <- rep(1, 3)

# Beregn x1*1 vektoren
x1_ones <- Ones * x1_bar

# Beregn afvigelsesvektoren
deviation <- y1 - x1_ones

# Vis resultater
cat(" Første kolonne (y1):", y1, "\n",
    "Gennemsnitsvektor (x1*1):", x1_ones, "\n",
    "Afvigelsesvektor (y1 - x1*1):", deviation, "\n")

# Kontroller at dekomponering er korrekt
cat("\nVerificering: sum(y1 - x1*1) =", sum(deviation), "\n")
```

Dekomponering af vektoren $y_1$ i gennemsnitsvektoren $x_1\mathbf{1}$ og afvigelsesvektoren $y_1 - x_1\mathbf{1}$ viser, at afvigelsesvektoren har sum 0, hvilket bekræfter, at gennemsnittet er korrekt beregnet.

# Opgave 3.4

Brug de seks observationer af variablen $X_1$ i millioner af enheder fra Tabel 1.1.

## Opgave a

Find projektionen på $\mathbf{1}' = [1,1,1,1,1,1]$.

```{r}
# Indlæsning af data
data("T1-1")
T1.1 <- tbl

# Udtræk X1 i millioner af enheder
X1 <- T1.1[,1] / 1000000

# Definering af en vektor af 1-taller
Ones <- rep(1, 6)

# Beregn projektionen på Ones
proj <- ((t(X1) %*% Ones) / (t(Ones) %*% Ones)) %*% Ones

# Vis resultater
cat("X1 værdier (i millioner):", X1, "\n")
cat("Projektionen på 1-vektoren:", proj, "\n")
```

Projektionen på $\mathbf{1}'$ vektoren er en vektor med samme værdi i alle positioner, og denne værdi er gennemsnittet af $X_1$. Dette fremgår af resultatet, hvor alle elementer i projektionsvektoren er ens.

## Opgave b

Beregn afvigelsesvektoren $y_1 - x_1\mathbf{1}$. Relatér dens længde til stikprøvestandardafvigelsen.

```{r}
# Beregn gennemsnittet af X1
x1_bar <- mean(X1)

# Beregn afvigelsesvektoren
Y <- X1 - Ones * x1_bar

# Beregn længden af afvigelsesvektoren i kvadrat
Ly2 <- sum(Y^2)

# Beregn standardafvigelsen baseret på længden
sdX <- sqrt(Ly2 / 6)

# Beregn standardafvigelsen direkte for sammenligning
# Bemærk: var() bruger n-1 i nævneren, så vi justerer med (n-1)/n
sdX_direct <- sqrt(var(X1) * 5 / 6)

# Vis resultater
cat("Afvigelsesvektor:", Y, "\n",
    "Længde af afvigelsesvektoren i kvadrat:", Ly2, "\n",
    "Stikprøvestandardafvigelse baseret på længden:", sdX, "\n",
    "Stikprøvestandardafvigelse beregnet direkte:", sdX_direct)
```

Længden af afvigelsesvektoren i kvadrat $\|y_1 - x_1\mathbf{1}\|^2$ er relateret til stikprøvevariansen ved $s^2 = \frac{1}{n}\|y_1 - x_1\mathbf{1}\|^2$. Stikprøvestandardafvigelsen er kvadratroden af denne værdi. De to beregnede værdier er ens, hvilket bekræfter relationen.

# Opgave 3.5

Beregn den generaliserede stikprøvevarians $|S|$ for følgende matricer.

## Opgave a

Matrix $X$ i Opgave 3.1.

```{r}
# Gendefiner matrix X fra Opgave 3.1
X <- matrix(c(9, 5, 1, 1, 3, 2), 3, 2)

# Beregn kovariansmatricen
S <- cov(X)

# Beregn den generaliserede stikprøvevarians (determinanten af S)
gen_var <- det(S)

# Vis resultater
cat("Kovariansmatrice S:\n")
print(S)
cat("\nGeneraliseret stikprøvevarians |S|:", gen_var, "\n")
```

## Opgave b

Matrix $X$ i Opgave 3.2.

```{r}
# Definering af matrix X fra Opgave 3.2
X <- matrix(c(3, 6, 3, 4, -2, 1), 3, 2, byrow = FALSE)
print(X)

# Beregn kovariansmatricen
S <- cov(X)

# Beregn den generaliserede stikprøvevarians (determinanten af S)
gen_var <- det(S)

# Vis resultater
cat("Kovariansmatrice S:\n", S, "\n",
    "Generaliseret stikprøvevarians |S|:", gen_var, "\n")
```

Den generaliserede stikprøvevarians $|S|$ er determinanten af kovariansmatricen. En lav værdi indikerer, at data er mere afhængige, mens en høj værdi indikerer større uafhængighed mellem variablerne.

# Opgave 3.6

Betragt matrix:

$$
X = \begin{bmatrix}
-1 & 3 & -2 \\
2 & 4 & 2 \\
5 & 2 & 3
\end{bmatrix}
$$

## Opgave a

Beregn matricen af afvigelser (residualer), $X - \mathbf{1}x'$. Er denne matrix af fuld rang? Forklar.

```{r}
# Definering af matrix X
X <- matrix(c(-1, 2, 5, 3, 4, 2, -2, 2, 3), 3, 3)

# Definering af en vektor af 1-taller
Ones <- rep(1, 3)

# Beregn afvigelsesmatricen
d <- X - Ones %*% t(colMeans(X))

# Undersøg rangen af afvigelsesmatricen
rank_d <- qr(d)$rank

# Vis resultater
# Afvigelsesmatrice
print(d)
cat("Rang af afvigelsesmatricen:", rank_d, "\n",
    "Antal kolonner i afvigelsesmatricen:", ncol(d), "\n")

# Undersøg lineær afhængighed mellem kolonnerne
cat("\nKontrol af lineær afhængighed:\n",
    "Sum af kolonne 1 og 2:", d[,1] + d[,2], "\n",
    "Kolonne 3:", d[,3])
```

Afvigelsesmatricen er ikke af fuld rang, da dens rang (2) er mindre end antallet af kolonner (3). Dette indikerer, at der er lineær afhængighed mellem kolonnerne. Ved inspektion kan vi se, at den tredje kolonne er summen af de første to kolonner, dvs. $d_3 = d_1 + d_2$. Dette er en typisk egenskab, da når vi centrerer en matrix, reduceres rangen med 1 på grund af begrænsningen om, at søjlesummerne skal være nul.

## Opgave b

Bestem $S$ og beregn den generaliserede stikprøvevarians $|S|$. Fortolk sidstnævnte geometrisk.

```{r}
# Beregn kovariansmatricen
S <- cov(X)

# Beregn den generaliserede stikprøvevarians (determinanten af S)
gen_var <- det(S)

# Udtræk kolonnerne som vektorer for visualisering
Y1 <- d[,1]
Y2 <- d[,2]
Y3 <- d[,3]

# Vis resultater
print(S)
cat("Generaliseret stikprøvevarians |S|:", gen_var, "\n")

# 3D Plot
plot3d(c(Y1[1],0), c(Y1[2],0), c(Y1[3],0), 
       xlab = "Y1", ylab = "Y2", zlab = "Y3", type = "l", 
       xlim = c(-4, 4), ylim = c(-4, 4), zlim = c(-4, 4))
plot3d(c(Y2[1], 0), c(Y2[2], 0), c(Y2[3], 0), add = T, type = "l")
plot3d(c(Y3[1], 0), c(Y3[2], 0), c(Y3[3], 0), add = T, type = "l")
```

Den generaliserede stikprøvevarians ∣S∣ er 0, hvilket viser, at mindst én variabel kan beregnes direkte ud fra de andre variable gennem en matematisk formel. Rent visuelt betyder det, at når vi plotter datapunkterne, fylder de ikke hele det mulige rum ud, men ligger i stedet samlet på et plan (eller en linje), som har færre dimensioner end det fulde antal variable.

## Opgave c

Ud fra resultaterne i (b) beregnes den totale stikprøvevarians. [Se (3-23).]

```{r}
# Beregn den totale stikprøvevarians (sporet af S)
total_var <- sum(diag(S))
total_var
```

Den totale stikprøvevarians er summen af varianserne for de enkelte variable (diagonalelementerne i S). Den repræsenterer den samlede variation i data og er positiv, selvom den generaliserede varians er nul, hvilket indikerer, at der stadig er variation i data, men at denne variation er begrænset til et underrum med lavere dimension.

# Opgave 3.7

Skitsér de solide ellipsoider $(x - \bar{x})'S^{-1}(x - \bar{x}) \leq 1$ [se (3-16)] for de tre matricer:

$$
S = \begin{bmatrix}
5 & 4 \\
4 & 5
\end{bmatrix}, \quad
S = \begin{bmatrix}
5 & -4 \\
-4 & 5
\end{bmatrix}, \quad
S = \begin{bmatrix}
3 & 0 \\
0 & 3
\end{bmatrix}
$$

(Bemærk at disse matricer har samme generaliserede varians $|S|$.)

```{r}
# Definer radius for alle ellipser
crad <- 1

# Matrix 1
S1 <- matrix(c(5, 4, 4, 5), 2, 2)
# Plot ellipsen
plot(c(-3, 3), c(-3, 3), type = "n", 
     main = "Ellipsoider med samme generaliserede varians",
     xlab = "X1", ylab = "X2", asp = 1)
f1 <- ellipse(c(0, 0), S1, radius = crad, col = "blue", 
              center.pch = FALSE, grid = FALSE, add = TRUE)

# Beregn egenvektorer og -værdier
eA1 <- eigen(S1)
# Plot hovedakserne
segments(0, 0, eA1$vectors[1,1] * sqrt(eA1$values[1] * crad), 
         eA1$vectors[2,1] * sqrt(eA1$values[1] * crad), col = "blue", lwd = 2)
segments(0, 0, eA1$vectors[1,2] * sqrt(eA1$values[2] * crad), 
         eA1$vectors[2,2] * sqrt(eA1$values[2] * crad), col = "blue", lwd = 2)

# Matrix 2
S2 <- matrix(c(5, -4, -4, 5), 2, 2)
# Plot ellipsen
f2 <- ellipse(c(0, 0), S2, radius = crad, col = "red", 
              center.pch = FALSE, grid = FALSE, add = TRUE)

# Beregn egenvektorer og -værdier
eA2 <- eigen(S2)
# Plot hovedakserne
segments(0, 0, eA2$vectors[1,1] * sqrt(eA2$values[1] * crad), 
         eA2$vectors[2,1] * sqrt(eA2$values[1] * crad), col = "red", lwd = 2)
segments(0, 0, eA2$vectors[1,2] * sqrt(eA2$values[2] * crad), 
         eA2$vectors[2,2] * sqrt(eA2$values[2] * crad), col = "red", lwd = 2)

# Matrix 3
S3 <- matrix(c(3, 0, 0, 3), 2, 2)
# Plot ellipsen
f3 <- ellipse(c(0, 0), S3, radius = crad, col = "green", 
              center.pch = FALSE, grid = FALSE, add = TRUE)

# Beregn egenvektorer og -værdier
eA3 <- eigen(S3)
# Plot hovedakserne
segments(0, 0, eA3$vectors[1,1] * sqrt(eA3$values[1] * crad), 
         eA3$vectors[2,1] * sqrt(eA3$values[1] * crad), col = "green", lwd = 2)
segments(0, 0, eA3$vectors[1,2] * sqrt(eA3$values[2] * crad), 
         eA3$vectors[2,2] * sqrt(eA3$values[2] * crad), col = "green", lwd = 2)

# Tilføj origo
points(0, 0, pch = "+")

# Tilføj forklaring
legend("topright", legend = c("S = [5 4; 4 5]", "S = [5 -4; -4 5]", "S = [3 0; 0 3]"), 
       col = c("blue", "red", "green"), lwd = 2)

# Beregn determinanterne for at vise, at de har samme generaliserede varians
det1 <- det(S1)
det2 <- det(S2)
det3 <- det(S3)

cat("Determinant af S1:", det1, "\n",
    "Determinant af S2:", det2, "\n",
    "Determinant af S3:", det3, "\n")
```

De tre ellipsoider repræsenterer konturerne af sandsynlighedsfordelinger med samme generaliserede varians (determinant), men forskellige former:

1. Den blå ellipsoide (S1) har positiv korrelation og er orienteret langs linjen fra nederste venstre til øverste højre.
2. Den røde ellipsoide (S2) har negativ korrelation og er orienteret langs linjen fra nederste højre til øverste venstre.
3. Den grønne ellipsoide (S3) er en cirkel, da der ikke er korrelation mellem variablerne.

Alle tre ellipsoider indeholder samme "volumen", da deres determinanter er ens, men de har forskellige former/orienteringer.

# Opgave 3.8

Givet:

$$
S = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} \quad \text{og} \quad
S = \begin{bmatrix}
1 & -1/2 & -1/2 \\
-1/2 & 1 & -1/2 \\
-1/2 & -1/2 & 1
\end{bmatrix}
$$

## Opgave a

Beregn den totale stikprøvevarians for hver $S$. Sammenlign resultaterne.

```{r}
# Definering af de to matricer
S1 <- diag(3)
S2 <- matrix(-0.5, 3, 3)
diag(S2) <- 1

# Beregn den totale stikprøvevarians (sporet af S)
total_var1 <- sum(diag(S1))
total_var2 <- sum(diag(S2))

# Vis resultater
cat("Total stikprøvevarians for S1:", total_var1, "\n",
    "Total stikprøvevarians for S2:", total_var2, "\n")
```

Den totale stikprøvevarians, beregnet som trace af kovariansmatricen, er ens for begge matricer (3). Dette betyder, at den samlede variation i data er den samme, selvom strukturen i korrelationen mellem variablerne er meget forskellig.

## Opgave b

Beregn den generaliserede stikprøvevarians for hver $S$, og sammenlign resultaterne. Kommenter eventuelle uoverensstemmelser fundet mellem del a og b.

```{r}
# Beregn den generaliserede stikprøvevarians (determinanten af S)
gen_var1 <- det(S1)
gen_var2 <- det(S2)

# Vis resultater
cat("Generaliseret stikprøvevarians for S1:", gen_var1, "\n",
    "Generaliseret stikprøvevarians for S2:", gen_var2, "\n")
```

For S1 (værdi = 1):

- Determinanten af kovariansmatricen er forskellig fra 0
- Der er ikke perfekt lineær afhængighed mellem variablerne
- Variablerne bidrager med unik information
- Datapunkterne spænder over det fulde dimensionale rum

For S2 (værdi = 0):

- Determinanten af kovariansmatricen er 0
- Der er perfekt lineær afhængighed mellem mindst nogle af variablerne
- Mindst én variabel kan udtrykkes som en lineær kombination af andre
- Datapunkterne ligger i et underrum med lavere dimension (f.eks. på en linje eller et plan)

Uoverensstemmelsen mellem del a og b viser, at den totale stikprøvevarians (trace) ikke tager højde for korrelationsstrukturen i data, mens den generaliserede stikprøvevarians (determinanten) gør. Selvom begge matricer har samme diagonalelementer (og dermed samme totale varians), har de meget forskellige strukturer i korrelation, hvilket afspejles i deres generaliserede varianser.

# Opgave 3.9

Følgende datamatrix indeholder data om testscorer, med $X_1$ = score på første test, $X_2$ = score på anden test, og $X_3$ = samlet score på de to tests:

$$
X = \begin{bmatrix}
12 & 17 & 29 \\
18 & 20 & 38 \\
14 & 16 & 30 \\
20 & 18 & 38 \\
16 & 19 & 35
\end{bmatrix}
$$

## Opgave a

Få den middelværdi-korrigerede datamatrix, og verificér at søjlerne er lineært afhængige. Specificér en $a' = [a_1, a_2, a_3]$ vektor, der etablerer den lineære afhængighed.

```{r}
# Definering af datamartix X
X <- matrix(c(12, 18, 14, 20, 16, 17, 20, 16, 18, 19, 29, 38, 30, 38, 35), 5, 3)

# Definering af en vektor af 1-taller
Ones <- rep(1, 5)

# Beregn middelværdi-korrigeret datamatrix
Xc <- X - Ones %*% t(colMeans(X))

# Specificer vektor a, der etablerer lineær afhængighed
a <- c(1, 1, -1)

# Verificer lineær afhængighed ved at beregne Xc %*% a
lin_dep <- Xc %*% a

# Vis resultater
# Middelværdi-korrigeret datamatrix
print(Xc)
# Vektor a
print(a)
# Xc %*% a 
print(lin_dep)
```

Den middelværdi-korrigerede datamatrix har lineært afhængige søjler, da Xc %*% a giver en vektor med værdier meget nær nul. Den lineære afhængighed er etableret med vektoren a = [1, 1, -1], hvilket indikerer, at den tredje kolonne er summen af de første to kolonner (efter middelværdi-korrektion).

## Opgave b

Få stikprøvekovariansmatricen $S$, og verificér at den generaliserede varians er nul. Vis også, at $Sa = 0$, så $a$ kan reskaleres til at være en egenvektor svarende til egenværdien nul.

```{r}
# Beregn stikprøvekovariansmatricen
S <- cov(X)

# Beregn den generaliserede varians (determinanten af S)
gen_var <- det(S)

# Verificer at Sa = 0
Sa <- S %*% a

# Vis resultater
# Stikprøvekovariansmatrice S
print(S)
cat("\nGeneraliseret varians |S|:", gen_var, "\n")
# S %*% a 
print(Sa)
```

Stikprøvekovariansmatricen har en generaliseret varians (determinant) på 0, hvilket bekræfter, at der er lineær afhængighed mellem variablerne. Desuden er S %*% a meget nær nul-vektoren, hvilket viser, at a er en egenvektor svarende til egenværdien 0.

## Opgave c

Verificér at den tredje kolonne af datamatricen er summen af de første to kolonner. Det vil sige, vis at der er lineær afhængighed, med $a_1 = 1$, $a_2 = 1$ og $a_3 = -1$.

```{r}
# Beregn summen af de første to kolonner
sum_col12 <- X[,1] + X[,2]

# Sammenlign med den tredje kolonne
diff <- sum_col12 - X[,3]

# Vis resultater
cat("Sum af kolonne 1 og 2:", sum_col12, "\n",
    "Kolonne 3:", X[,3], "\n",
    "Forskel :", diff, "\n")
```

Resultaterne viser, at summen af de første to kolonner er lig med den tredje kolonne (differensen er 0 for alle rækker). Dette bekræfter den lineære afhængighed med a = [1, 1, -1], hvor a₁ = 1, a₂ = 1 og a₃ = -1. Dette giver mening i konteksten af dataene, da X₃ repræsenterer den samlede score, som er summen af scorerne fra de to individuelle tests.

# Opgave 3.10

Når den generaliserede varians er nul, er det søjlerne i den middelværdi-korrigerede datamatrix $X_c = X - \mathbf{1}x'$, der er lineært afhængige, ikke nødvendigvis søjlerne i selve datamatricen. Givet data:

$$
X = \begin{bmatrix}
3 & 1 & 0 \\
6 & 4 & 6 \\
4 & 2 & 2 \\
7 & 0 & 3 \\
5 & 3 & 4
\end{bmatrix}
$$

## Opgave a

Få den middelværdi-korrigerede datamatrix, og verificér at søjlerne er lineært afhængige. Specificér en $a' = [a_1, a_2, a_3]$ vektor, der etablerer afhængigheden.

```{r}
# Definering af datamatrix X
X <- matrix(c(3, 6, 4, 7, 5, 1, 4, 2, 0, 3, 0, 6, 2, 3, 4), 5, 3)

# Definering af en vektor af 1-taller
Ones <- rep(1, 5)

# Beregn middelværdi-korrigeret datamatrix
Xc <- X - Ones %*% t(colMeans(X))

# Vis middelværdi-korrigeret datamatrix
print(Xc)

# Find en vektor a der etablerer lineær afhængighed
# Baseret på et kvalificeret gæt, prøver vi med a = [1, 1, -1]
a <- c(1, 1, -1)

# Beregn Xc %*% a for at verificere lineær afhængighed
result <- Xc %*% a

# Xc %*% a (bør være meget tæt på nul hvis lineær afhængighed)
print(result)

# Verificér at resultatets numeriske værdier er meget tæt på nul
cat("Summen af absolutværdier:", sum(abs(result)), "\n")
```

Ovenstående resultater viser, at søjlerne i den middelværdi-korrigerede datamatrix er lineært afhængige, da vektoren $a = [1, 1, -1]$ giver en lineær kombination, der resulterer i en vektor med nul. Dette betyder, at den tredje søjle i Xc kan udtrykkes som en lineær kombination af de første to søjler: $Xc_3 = Xc_1 + Xc_2$.

## Opgave b

Få stikprøvekovariansmatricen $S$, og verificér at den generaliserede varians er nul.

```{r}
# Beregn stikprøvekovariansmatricen
S <- cov(X)

# Vis kovariansmatricen
print(S)

# Beregn den generaliserede varians (determinanten af S)
gen_var <- det(S)
cat("Generaliseret varians |S| =", gen_var, "\n")

# Verificer at S %*% a er nær nul
Sa <- S %*% a
# S %*% a
print(Sa)
```

Den generaliserede varians |S| er meget tæt på nul (inden for numerisk præcision), hvilket bekræfter at søjlerne i den middelværdi-korrigerede datamatrix er lineært afhængige. Yderligere bekræftes dette ved at S %*% a giver en vektor, der er på nul-vektoren, hvilket viser at a er en egenvektor for S med en tilhørende egenværdi tæt på 0.

## Opgave c

Vis at søjlerne i datamatricen er lineært uafhængige i dette tilfælde.

```{r}
# Beregn rangen af den originale datamatrix
rank_X <- qr(X)$rank

# Vis rangen
cat("Rang af den originale datamatrix X:", rank_X, "\n",
    "Antal kolonner i X:", ncol(X), "\n")
```

I modsætning til den middelværdi-korrigerede matrix er søjlerne i den originale datamatrix X lineært uafhængige. Dette kan ses ved at:

1. Rangen af X er 3, hvilket er lig med antallet af søjler, hvilket indikerer fuld rang

Dette illustrerer en vigtig egenskab: Selv når søjlerne i den middelværdi-korrigerede matrix Xc er lineært afhængige, kan søjlerne i den originale datamatrix X stadig være lineært uafhængige. Den generaliserede varians er nul på grund af de lineære afhængigheder i Xc, ikke nødvendigvis i X selv.

# Opgave 3.11

Brug stikprøvekovariansen opnået i Eksempel 3.7 til at verificere (3-29) og (3-30), som angiver at $R = D^{-1/2}SD^{-1/2}$ og $D^{1/2}RD^{1/2} = S$.

```{r}
# Definering af stikprøvekovariansmatricen fra Eksempel 3.7
S <- matrix(c(252.04, -68.43, -68.43, 123.67), 2, 2)

# Vis kovariansmatricen
print(S)

# Beregn D (diagonal matrix med varianserne)
D <- diag(diag(S))
print(D)

# Beregn D^(-1/2)
Dhalf_inv <- diag(1/sqrt(diag(S)))
print(Dhalf_inv)

# Beregn korrelationsmatricen R ved brug af formel (3-29)
R <- Dhalf_inv %*% S %*% Dhalf_inv
print(R)

# Beregn D^(1/2)
Dhalf <- diag(sqrt(diag(S)))
print(Dhalf)

# Beregn S ved brug af formel (3-30)
S_restored <- Dhalf %*% R %*% Dhalf
print(S_restored)

# Verificer at R har 1'er på diagonalen (som en korrelationsmatrix skal have)
cat("Diagonalelementer i R:", diag(R), "\n")

# Verificer at S_restored matcher den originale S
diff <- sum(abs(S - S_restored))
cat("Summen af absolutte forskelle mellem S og den rekonstruerede S:", diff, "\n")

# Verificer relationen mellem determinanterne
det_S <- det(S)
det_R <- det(R)
det_D <- det(D)
cat("\nDeterminant af S:", det_S, "\n",
    "Determinant af D:", det_D, "\n",
    "Determinant af R:", det_R, "\n",
    "S[1,1] * S[2,2] * det(R):", S[1,1] * S[2,2] * det_R, "\n",
    "Sammenligning: |S| = |D| * |R| ?", abs(det_S - det_D * det_R) < 1e-10, "\n")
```

Ovenstående beregninger verificerer følgende relationer:

1. **Formel (3-29)**: $R = D^{-1/2}SD^{-1/2}$
   Vi har beregnet korrelationsmatricen R ved brug af denne formel, og som forventet har den 1'er på diagonalen, hvilket er karakteristisk for en korrelationsmatrix.

2. **Formel (3-30)**: $D^{1/2}RD^{1/2} = S$
   Vi har rekonstrueret kovariansmatricen S ved at bruge denne formel, og den rekonstruerede matrix matcher den originale S (med en meget lille numerisk forskel).

3. **Determinant-relation**: $|S| = |D|·|R|$
   Vi har verificeret at determinanten af S er lig med produktet af determinanterne af D og R, hvilket bekræfter relationen mellem generaliserede varianser.

Disse resultater bekræfter de matematiske relationer mellem kovariansmatricen S, korrelationsmatricen R og den diagonale matrix D med varianserne på diagonalen.

# Opgave 3.14

Betragt datamatricen $X$ i Opgave 3.1. Vi har $n = 3$ observationer på $p = 2$ variable $X_1$ og $X_2$. Konstruér de lineære kombinationer:

$$
c'X = [-1 \; 2]\begin{bmatrix} X_1 \\ X_2 \end{bmatrix}  = -X_1 + 2X_2
$$

$$
b'X = [2 \; 3]\begin{bmatrix} X_1 \\ X_2 \end{bmatrix}  = 2X_1 + 3X_2
$$

## Opgave a

Evaluér stikprøvegennemsnit, varianser og kovarians af $b'X$ og $c'X$ fra første principper. Det vil sige, beregn de observerede værdier af $b'X$ og $c'X$, og brug derefter formlerne for stikprøvegennemsnit, varians og kovarians.

```{r}
# Definering af matrix X fra Opgave 3.1
X <- matrix(c(9, 5, 1, 1, 3, 2), 3, 2)

# Vis datamatricen
print(X)

# Definering af vektorerne c og b
cvec <- c(-1, 2)
bvec <- c(2, 3)

# Beregn de lineære kombinationer c'X og b'X
cX <- X %*% cvec
bX <- X %*% bvec

# Beregn kombinationerne for hver observation
# c'X værdier: c'X = -X1 + 2*X2
for (i in 1:nrow(X)) {
  cat("Observation", i, ": -", X[i, 1], " + 2 *", X[i, 2], " = ", cX[i], "\n")
}

# b'X værdier: b'X = 2*X1 + 3*X2
for (i in 1:nrow(X)) {
  cat("Observation", i, ": 2*", X[i, 1], " + 3 *", X[i, 2], " = ", bX[i], "\n")
}

# Beregn stikprøvegennemsnit af cX og bX
cX_mean <- mean(cX)
bX_mean <- mean(bX)

# Beregn varianser af cX og bX
cX_var <- var(cX) * (nrow(X) - 1) / nrow(X) # Juster til stikprøvevarians med n
bX_var <- var(bX) * (nrow(X) - 1) / nrow(X) # i stedet for (n-1) i nævneren

# Beregn kovarians mellem cX og bX
cX_bX_cov <- cov(cX, bX) * (nrow(X) - 1) / nrow(X) # Juster til stikprøvekovarians

# Vis resultater fra første principper
cat("\nResultater fra første principper:\n",
    "Stikprøvegennemsnit af c'X:", cX_mean, "\n",
    "Stikprøvegennemsnit af b'X:", bX_mean, "\n",
    "Stikprøvevarians af c'X:", cX_var, "\n",
    "Stikprøvevarians af b'X:", bX_var, "\n",
    "Stikprøvekovarians mellem c'X og b'X:", cX_bX_cov, "\n")
```

I denne del har vi beregnet de lineære kombinationer c'X og b'X for hver observation og derefter fundet stikprøvegennemsnit, varianser og kovarians direkte ud fra disse værdier. Dette er en beregning "fra første principper", hvor vi anvender de grundlæggende definitioner på de transformerede data.

## Opgave b

Beregn stikprøvegennemsnit, varianser og kovarians af $b'X$ og $c'X$ ved brug af (3-36). Sammenlign resultaterne i (a) og (b).

```{r}
# Beregn stikprøvegennemsnit og kovariansmatrice for X
xm <- colMeans(X)
S <- cov(X) * (nrow(X) - 1) / nrow(X) # Juster til stikprøvekovarians

# Beregn stikprøvegennemsnit af c'X og b'X ved brug af (3-36)
cX_mean_36 <- t(cvec) %*% xm
bX_mean_36 <- t(bvec) %*% xm

# Beregn varianser og kovarians ved brug af (3-36)
cX_var_36 <- t(cvec) %*% S %*% cvec
bX_var_36 <- t(bvec) %*% S %*% bvec
cX_bX_cov_36 <- t(cvec) %*% S %*% bvec

# Vis kovariansmatricen for X
print(S)

# Vis resultater ved brug af (3-36)
cat("\nResultater ved brug af (3-36):\n",
    "Stikprøvegennemsnit af c'X:", cX_mean_36[1,1], "\n",
    "Stikprøvegennemsnit af b'X:", bX_mean_36[1,1], "\n",
    "Stikprøvevarians af c'X:", cX_var_36[1,1], "\n",
    "Stikprøvevarians af b'X:", bX_var_36[1,1], "\n",
    "Stikprøvekovarians mellem c'X og b'X:", cX_bX_cov_36[1,1], "\n")

# Sammenlign resultaterne fra (a) og (b)
cat("\nSammenligning af resultater:\n",
    "Stikprøvegennemsnit af c'X - Forskel:", abs(cX_mean - cX_mean_36[1,1]), "\n",
    "Stikprøvegennemsnit af b'X - Forskel:", abs(bX_mean - bX_mean_36[1,1]), "\n",
    "Stikprøvevarians af c'X - Forskel:", abs(cX_var - cX_var_36[1,1]), "\n",
    "Stikprøvevarians af b'X - Forskel:", abs(bX_var - bX_var_36[1,1]), "\n",
    "Stikprøvekovarians - Forskel:", abs(cX_bX_cov - cX_bX_cov_36[1,1]), "\n")
```

I denne del har vi beregnet de samme statistiske egenskaber ved hjælp af formel (3-36), som giver os:

1. $E[c'X] = c'E[X] = c'\bar{x}$
2. $Var[c'X] = c'Sc$
3. $Cov[c'X, b'X] = c'Sb$

Sammenligning af resultaterne viser, at begge metoder giver identiske resultater (med meget små numeriske forskelle). Dette bekræfter formel (3-36), som giver os en effektiv måde at beregne statistiske egenskaber for lineære kombinationer af multivariate data.

Formel (3-36) er særligt nyttig i praksis, fordi den giver os mulighed for at beregne disse egenskaber direkte fra stikprøvegennemsnittet og kovariansmatricen uden at skulle beregne de faktiske lineære kombinationer for hver observation.