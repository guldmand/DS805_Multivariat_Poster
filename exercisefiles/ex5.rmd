---
title: "Kapitel 4: Den Multivariate Normalfordeling - Øvelser"
author: "Kasper Veje Jakobsen"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    number_sections: false
    theme: flatly
    highlight: tango
    df_print: paged
    code_folding: show
---

```{r}
knitr::opts_chunk$set(echo = TRUE)
library(st514)
library(MASS)
library(car)
options(scipen = 999, digits = 4)
rm(list=ls())
```

# Opgave 4.1

Betragt en bivariat normalfordeling med $\mu_1 = 1$, $\mu_2 = 3$, $\sigma_{11} = 2$, $\sigma_{22} = 1$ og $\rho_{12} = -.8$.

## Opgave 4.1.a

Skriv den bivariate normaltæthed ud.

*Hint: Den bivariate normaltæthed er givet ved formlen:*

$$f(x_1, x_2) = \frac{1}{2\pi\sqrt{|\Sigma|}} \exp\left\{-\frac{1}{2}(x-\mu)'\Sigma^{-1}(x-\mu)\right\}$$

For at udregne tæthedsfunktionen skal vi først bestemme kovariansmatricen $\Sigma$ og dens determinant.

Givet er:
- $\mu_1 = 1$ og $\mu_2 = 3$, hvilket giver $\mu = \begin{bmatrix} 1 \\ 3 \end{bmatrix}$
- $\sigma_{11} = 2$ og $\sigma_{22} = 1$
- $\rho_{12} = -0.8$

Kovariansen $\sigma_{12}$ beregnes som $\sigma_{12} = \rho_{12}\sqrt{\sigma_{11}\sigma_{22}} = -0.8 \cdot \sqrt{2 \cdot 1} = -0.8\sqrt{2}$

Dermed er kovariansmatricen:

$$\Sigma = \begin{bmatrix} \sigma_{11} & \sigma_{12} \\ \sigma_{21} & \sigma_{22} \end{bmatrix} = \begin{bmatrix} 2 & -0.8\sqrt{2} \\ -0.8\sqrt{2} & 1 \end{bmatrix}$$

Determinanten af $\Sigma$ er:

$$|\Sigma| = \sigma_{11}\sigma_{22} - \sigma_{12}\sigma_{21} = 2 \cdot 1 - (-0.8\sqrt{2})^2 = 2 - 0.64 \cdot 2 = 2 - 1.28 = 0.72$$

For at finde $\Sigma^{-1}$ benytter vi formlen for inverse af en $2 \times 2$ matrix:

$$\Sigma^{-1} = \frac{1}{|\Sigma|} \begin{bmatrix} \sigma_{22} & -\sigma_{12} \\ -\sigma_{21} & \sigma_{11} \end{bmatrix} = \frac{1}{0.72} \begin{bmatrix} 1 & 0.8\sqrt{2} \\ 0.8\sqrt{2} & 2 \end{bmatrix}$$

$$\Sigma^{-1} = \begin{bmatrix} \frac{1}{0.72} & \frac{0.8\sqrt{2}}{0.72} \\ \frac{0.8\sqrt{2}}{0.72} & \frac{2}{0.72} \end{bmatrix}$$

Den bivariate normaltæthed bliver derfor:

$$f(x_1, x_2) = \frac{1}{2\pi\sqrt{0.72}} \exp\left\{-\frac{1}{2}\begin{bmatrix} x_1 - 1 & x_2 - 3 \end{bmatrix} \begin{bmatrix} \frac{1}{0.72} & \frac{0.8\sqrt{2}}{0.72} \\ \frac{0.8\sqrt{2}}{0.72} & \frac{2}{0.72} \end{bmatrix} \begin{bmatrix} x_1 - 1 \\ x_2 - 3 \end{bmatrix}\right\}$$

$$f(x_1, x_2) = \frac{1}{2\pi\sqrt{0.72}} \exp\left\{-\frac{1}{2}\left[\frac{(x_1-1)^2}{0.72} + \frac{2 \cdot 0.8\sqrt{2}(x_1-1)(x_2-3)}{0.72} + \frac{2(x_2-3)^2}{0.72}\right]\right\}$$

Dette kan forenkles til:

$$f(x_1, x_2) = \frac{1}{2\pi\sqrt{0.72}} \exp\left\{-\frac{1}{2}\left[\frac{(x_1-1)^2}{0.72} + \frac{2\sqrt{2} \cdot 0.8(x_1-1)(x_2-3)}{0.72} + \frac{2(x_2-3)^2}{0.72}\right]\right\}$$


## Opgave 4.1.b

Skriv udtrykket for den kvadrerede statistiske afstand $(x - \mu)'\Sigma^{-1}(x - \mu)$ som en kvadratisk funktion af $x_1$ og $x_2$.

Den kvadrerede statistiske afstand er det udtryk, der optræder i eksponenten af tæthedsfunktionen (ganget med $-\frac{1}{2}$).

Fra del a har vi beregnet $\Sigma^{-1}$:

$$\Sigma^{-1} = \begin{bmatrix} \frac{1}{0.72} & \frac{0.8\sqrt{2}}{0.72} \\ \frac{0.8\sqrt{2}}{0.72} & \frac{2}{0.72} \end{bmatrix}$$

Den kvadrerede statistiske afstand er:

$$(x - \mu)'\Sigma^{-1}(x - \mu) = \begin{bmatrix} x_1 - 1 & x_2 - 3 \end{bmatrix} \begin{bmatrix} \frac{1}{0.72} & \frac{0.8\sqrt{2}}{0.72} \\ \frac{0.8\sqrt{2}}{0.72} & \frac{2}{0.72} \end{bmatrix} \begin{bmatrix} x_1 - 1 \\ x_2 - 3 \end{bmatrix}$$

Dette giver:

$$(x - \mu)'\Sigma^{-1}(x - \mu) = \frac{(x_1-1)^2}{0.72} + \frac{2 \cdot 0.8\sqrt{2}(x_1-1)(x_2-3)}{0.72} + \frac{2(x_2-3)^2}{0.72}$$

Dette kan omskrives til:

$$(x - \mu)'\Sigma^{-1}(x - \mu) = \frac{1}{0.72}(x_1-1)^2 + \frac{2\sqrt{2} \cdot 0.8}{0.72}(x_1-1)(x_2-3) + \frac{2}{0.72}(x_2-3)^2$$

For at få numeriske værdier, beregner vi koefficienterne:

$$\frac{1}{0.72} \approx 1.389$$

$$\frac{2\sqrt{2} \cdot 0.8}{0.72} \approx \frac{2.26}{0.72} \approx 3.139$$

$$\frac{2}{0.72} \approx 2.778$$

Den kvadrerede statistiske afstand er dermed:

$$(x - \mu)'\Sigma^{-1}(x - \mu) \approx 1.389(x_1-1)^2 + 3.139(x_1-1)(x_2-3) + 2.778(x_2-3)^2$$

# Opgave 4.3

Lad $X$ være $N_3(\mu, \Sigma)$ med $\mu' = [-3, 1, 4]$ og

$$
\Sigma = \begin{bmatrix}
1 & -2 & 0 \\
-2 & 5 & 0 \\
0 & 0 & 2
\end{bmatrix}
$$

Hvilke af følgende stokastiske variable er uafhængige? Forklar.

*Hint: I en multivariat normalfordeling er kovarians lig 0 ensbetydende med uafhængighed.*

## Opgave 4.3.a

$X_1$ og $X_2$

For at undersøge om $X_1$ og $X_2$ er uafhængige, ser vi på kovariansen mellem dem i kovariansmatricen $\Sigma$.

Kovariansen mellem $X_1$ og $X_2$ er $\sigma_{12} = -2$.

Da $\sigma_{12} \neq 0$, er $X_1$ og $X_2$ ikke uafhængige. I en multivariat normalfordeling kræver uafhængighed, at kovariansen er 0.

## Opgave 4.3.b

$X_2$ og $X_3$

For at undersøge om $X_2$ og $X_3$ er uafhængige, ser vi på kovariansen mellem dem i kovariansmatricen $\Sigma$.

Kovariansen mellem $X_2$ og $X_3$ er $\sigma_{23} = 0$.

Da $\sigma_{23} = 0$, er $X_2$ og $X_3$ uafhængige. I en multivariat normalfordeling medfører kovarians lig 0, at de respektive variable er uafhængige.

## Opgave 4.3.c

$(X_1, X_2)$ og $X_3$

For at undersøge om $(X_1, X_2)$ og $X_3$ er uafhængige, skal vi se på kovarianser mellem $X_1$ og $X_3$ samt mellem $X_2$ og $X_3$.

Fra kovariansmatricen har vi:
- Kovariansen mellem $X_1$ og $X_3$ er $\sigma_{13} = 0$
- Kovariansen mellem $X_2$ og $X_3$ er $\sigma_{23} = 0$

Da både $\sigma_{13} = 0$ og $\sigma_{23} = 0$, er $(X_1, X_2)$ og $X_3$ uafhængige. I en multivariat normalfordeling er to grupper af variable uafhængige, hvis alle par af variable på tværs af grupperne har kovarians lig 0.

# Opgave 4.6

Lad $X$ være fordelt som $N_3(\mu, \Sigma)$, hvor $\mu' = [1, -1, 2]$ og

$$
\Sigma = \begin{bmatrix}
4 & 0 & -1 \\
0 & 5 & 0 \\
-1 & 0 & 2
\end{bmatrix}
$$

Hvilke af følgende stokastiske variable er uafhængige? Forklar.

## Opgave 4.6.a

$X_1$ og $X_2$

Fra den givne kovariansmatrix har vi $\sigma_{12} = 0$.

Da kovariansen mellem $X_1$ og $X_2$ er 0, og vi ved fra teorien om multivariat normalfordeling, at kovarians lig 0 er ensbetydende med uafhængighed, kan vi konkludere, at $X_1$ og $X_2$ er uafhængige.

## Opgave 4.6.b

$X_1$ og $X_3$

Fra den givne kovariansmatrix har vi $\sigma_{13} = -1$.

Da kovariansen mellem $X_1$ og $X_3$ er $-1 \neq 0$, er $X_1$ og $X_3$ ikke uafhængige.

## Opgave 4.6.c

$X_2$ og $X_3$

Fra den givne kovariansmatrix har vi $\sigma_{23} = 0$.

Da kovariansen mellem $X_2$ og $X_3$ er 0, kan vi konkludere, at $X_2$ og $X_3$ er uafhængige.

## Opgave 4.6.d

$(X_1, X_3)$ og $X_2$

For at undersøge uafhængigheden mellem den bivariate vektor $(X_1, X_3)$ og den skalare variabel $X_2$, skal vi undersøge, om $X_1$ og $X_2$ samt $X_3$ og $X_2$ er parvis uafhængige.

Fra tidligere delspørgsmål ved vi:
- $\sigma_{12} = 0$ $\Rightarrow$ $X_1$ og $X_2$ er uafhængige
- $\sigma_{23} = 0$ $\Rightarrow$ $X_3$ og $X_2$ er uafhængige

Da både $X_1$ og $X_2$ er uafhængige, og $X_3$ og $X_2$ er uafhængige, kan vi konkludere, at $(X_1, X_3)$ og $X_2$ er uafhængige. Dette skyldes, at for multivariat normalfordeling gælder det, at hvis to grupper af variable er uafhængige, så er alle par af variable på tværs af grupperne uafhængige.

## Opgave 4.6.e

$X_1$ og $X_1 + 3X_2 - 2X_3$

Lad os kalde $Y = X_1 + 3X_2 - 2X_3$. For at undersøge uafhængigheden mellem $X_1$ og $Y$, skal vi beregne $\text{Cov}(X_1, Y)$.

$$\begin{align*}
\text{Cov}(X_1, Y) &= \text{Cov}(X_1, X_1 + 3X_2 - 2X_3) \\
&= \text{Cov}(X_1, X_1) + 3\text{Cov}(X_1, X_2) - 2\text{Cov}(X_1, X_3) \\
&= \text{Var}(X_1) + 3\sigma_{12} - 2\sigma_{13} \\
&= \sigma_{11} + 3 \cdot 0 - 2 \cdot (-1) \\
&= 4 + 0 + 2 \\
&= 6
\end{align*}$$

Da $\text{Cov}(X_1, Y) = 6 \neq 0$, er $X_1$ og $Y = X_1 + 3X_2 - 2X_3$ ikke uafhængige.

## Konklusion

Opsummeret har vi følgende resultater:

1. $X_1$ og $X_2$ er uafhængige, fordi $\sigma_{12} = 0$.
2. $X_1$ og $X_3$ er ikke uafhængige, fordi $\sigma_{13} = -1 \neq 0$.
3. $X_2$ og $X_3$ er uafhængige, fordi $\sigma_{23} = 0$.
4. $(X_1, X_3)$ og $X_2$ er uafhængige, fordi $X_1$ og $X_2$ samt $X_3$ og $X_2$ er parvis uafhængige.
5. $X_1$ og $X_1 + 3X_2 - 2X_3$ er ikke uafhængige, fordi $\text{Cov}(X_1, X_1 + 3X_2 - 2X_3) = 6 \neq 0$.


# Opgave 4.18

Find maximum likelihood-estimaterne af den $2 \times 1$ middelværdivektor $\mu$ og den $2 \times 2$ kovariansmatrix $\Sigma$ baseret på den tilfældige stikprøve

$$
X = \begin{bmatrix}
3 & 6 \\
4 & 4 \\
5 & 7 \\
4 & 7
\end{bmatrix}
$$

fra en bivariat normalfordeling.

*Hint: Maximum likelihood-estimaterne for $\mu$ og $\Sigma$ er givet ved stikprøvegennemsnittet og stikprøvekovariansmatricen.*

```{r}
# Definer data matricen
x <- matrix(c(3, 4, 5, 4, 6, 4, 7, 7), 4, 2)

# Beregn ML-estimater
mu_hat <- colMeans(x)
Sigma_hat <- cov(x) * (nrow(x) - 1) / nrow(x)  # Konverter fra stikprøve til ML-estimat

# ML Estimat for Middelværdi
print(mu_hat)

# ML Estimat for kovariansmatrice
print(Sigma_hat)
```

# Opgave 4.23

Betragt de årlige afkast (inklusive udbytter) på Dow-Jones industrigennemsnit for årene 1996-2005. Disse data, ganget med 100, er
-0.6, 3.1, 25.3, -16.8, -7.1, -6.2, 25.2, 22.6, 26.0.

## Opgave 4.23.a

Konstruer et Q-Q plot. Synes dataene at være normalfordelte? Forklar.

```{r}
# Definer data
x <- c(-0.6, 3.1, 25.3, -16.8, -7.1, -6.2, 25.2, 22.6, 26)

# Konstruer Q-Q plot
qqnorm(x, main = "Normal Q-Q Plot for Dow-Jones årlige afkast")
qqline(x)
```
Q-Q plottet viser afvigelser fra normalfordelingen, for de ekstreme værdier. De fleste punkter i midten af data følger den rette linje nogenlunde.


# Opgave 4.24

Øvelse 1.4 indeholder data om tre variable for verdens 10 største virksomheder pr. april 2005.

## Opgave 4.24.a

Konstruer Q-Q plots. Synes disse data at være normalfordelte? Forklar.

```{r}
# Indlæs data
data("P1-4")

# Lav første QQ-plot
qqnorm(tbl[,1])
qqline(tbl[,1])

# Lav andet QQ-plot
qqnorm(tbl[,2])
qqline(tbl[,2])
```

# Opgave 4.25

Henvis til dataene for verdens 10 største virksomheder fra opgave 4.24. Konstruer et $\chi^2$ plot ved brug af alle tre variable. $\chi^2$ kvantilerne er:

0.3518, 0.7978, 1.2125, 1.6416, 2.1095, 2.6430, 3.2831, 4.1083, 5.3170, 7.8147

*Hint: Et $\chi^2$ plot er et plot af de ordnede kvadrerede statistiske afstande mod $\chi^2$ kvantiler.*

```{r}
# Data til matrice
X <- as.matrix(tbl)

# Beregn middelværdivektor
x_mean <- colMeans(X)

# Beregn kovariansmatricen
S <- cov(X)

# Antal observationer
n <- nrow(X)

# Beregn de kvadrerede statistiske afstande (Mahalanobis)
D2 <- rep(0, n)
for (i in 1:n) {
  D2[i] <- t(X[i,] - x_mean) %*% solve(S) %*% (X[i,] - x_mean)
}

# Alternativ metode (kommenteret ud da vi allerede har beregnet D2)
D2_alt <- mahalanobis(X, x_mean, S)

# Antal variable
p <- ncol(x)

# $\chi^2$ kvantiler
chisq_quantiles <- c(0.3518, 0.7978, 1.2125, 1.6416, 2.1095, 2.6430, 3.2831, 4.1083, 5.3170, 7.8147)

# Sorter D2 værdier
D2_sorted <- sort(D2)

# $\chi^2$ plot
plot(chisq_quantiles, D2_sorted, 
     xlab = bquote("Kvantiler af " ~ chi[.(p)]^2), 
     ylab = "Mahalanobis afstande",
     main = bquote("Q-Q plot af Mahalanobis" * ~ D^2 * " vs. kvantiler af" * ~ chi[.(p)]^2))
abline(0, 1, lty = 2)

cat("De beregnede Mahalanobis afstande er:", D2, "\n",
    "Alternativ metode:", D2_alt, "\n")
```

$\chi^2$ plottet viser forholdet mellem de ordnede Mahalanobis afstande og $\chi^2$ kvantilerne. Hvis punkterne ligger tæt på 45-graders linjen (den stiplede linje), indikerer det, at data er multivariat normalfordelt. I dette tilfælde ser vi en rimelig god tilpasning til linjen, hvilket understøtter antagelsen om multivariat normalitet. En enkelt ekstrem værdi har dog en stor afvigelse fra linjen.

# Opgave 4.26

Øvelse 1.2 giver alderen $x_1$, målt i år, samt salgsprisen $x_2$, målt i tusinder af dollars, for $n = 10$ brugte biler. Disse data er gengivet som følger:

| $x_1$ | 1     | 2     | 3     | 3     | 4     | 5     | 6    | 8    | 9    | 11   |
| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ---- | ---- | ---- | ---- |
| $x_2$ | 18.95 | 19.00 | 17.95 | 15.54 | 14.00 | 12.95 | 8.94 | 7.49 | 6.00 | 3.99 |

## Opgave 4.26.a

Brug resultaterne fra Øvelse 1.2 til at beregne de kvadrerede statistiske afstande $(x_j - \bar{x})'S^{-1}(x_j - \bar{x})$, $j = 1, 2, \dots, 10$, hvor $x_j' = [x_{j1}, x_{j2}]$.

*Hint: Indlæs dataene og beregn middelværdivektoren og kovariansmatricen, hvorefter de kvadrerede statistiske afstande kan beregnes.*

```{r}
# Definer data
X <- matrix(c(1, 2, 3, 3, 4, 5, 6, 8, 9, 11, 
              18.95, 19, 17.95, 15.54, 14, 12.95, 8.94, 7.49, 6, 3.99), 10, 2)

# Beregn middelværdivektor
x_mean <- colMeans(X)

# Beregn kovariansmatrix
S <- cov(X)

# Antal observationer
n <- nrow(X)

# Beregn de kvadrerede statistiske afstande
D2 <- mahalanobis(X, x_mean, S)
print(D2)
```

## Opgave 4.26.b

Med brug af afstandene i del a, bestem andelen af observationer, der falder inden for den estimerede 50% sandsynlighedskontur for en bivariat normalfordeling.

*Hint: Den 50% sandsynlighedskontur svarer til en $\chi^2$ værdi med 2 frihedsgrader.*

```{r}
# Antal frihedsgrader (antal variable)
p <- ncol(X)

# Find 50% kvantilen for chi-i-anden fordelingen med p frihedsgrader
chisq_50pct <- qchisq(0.5, p)

# Find observationer der falder inden for 50% sandsynlighedskonturen
within_50pct <- which(D2 < chisq_50pct)

# Beregn andelen
proportion_within_50pct <- length(within_50pct) / n
cat("50% kvantilen for chi-i-anden fordelingen med", p, "frihedsgrader er:", chisq_50pct, "\n",
    "Observationer der falder inden for 50% sandsynlighedskonturen:", within_50pct, "\n",
    "Andelen af observationer inden for 50% sandsynlighedskonturen:", proportion_within_50pct, "\n",
    "Dette svarer til", length(within_50pct), "ud af", n, "observationer.\n")
```

## Opgave 4.26.c

Konstruer et $\chi^2$ plot.

```{r}
# chi-i-anden plot
qqplot(qchisq(ppoints(n, a = 0.5), df = p), D2,
       ylab = "Mahalanobis distances",
       xlab = bquote("Quantiles of " ~ chi[.(p)]^2),
       main = bquote("Q-Q plot of Mahalanobis" * ~ D^2 * 
                       " vs. quantiles of" * ~ chi[.(p)]^2))
abline(0,1)
```

# Opgave 4.40

Betragt dataene for nationalparker i Øvelse 1.27. For denne opgave, brug følgende simulerede data:

```{r}
# Indlæs data fra bibliotekets datasæt
data("T1-11")
# Omdøb kolonner og rækker
colnames(tbl) <- c("Size", "Visitors")
rownames(tbl) <- c("Arcadia", "Bryce Canyen",
                   "Cuyahoga Valley", "Everglades",
                   "Grand Canyon", "Grand Teton",
                   "Great Smoky", "Hot Springs",
                   "Olympic", "Mount Rainier",
                   "Rocky Mountain", "Shenandoah",
                   "Yellowstone", "Yosemite", "Zion")

# Vis datasættet
print(tbl)
```

## Opgave 4.40.a

Kommenter eventuelle outliers i et scatter plot af de originale variable.

```{r}
# Scatter plot
plot(tbl, xlab = "Size (1000 acres)", ylab = "Visitors (millions)")

# Find parken med flest besøgende
unusual <- which(tbl[, 2] == max(tbl[, 2]))

# Tilføj navn til parken med flest besøgende
text(tbl[unusual, 1], tbl[unusual, 2] - 0.5, rownames(tbl)[unusual])

# Beregn korrelation med og uden outlier
correlation_all <- round(cor(tbl), 3)
correlation_without_outlier <- round(cor(tbl[-unusual, ]), 3)

# Vis korrelationer
cat("Korrelation mellem størrelse og besøgende (alle parker):", correlation_all[1,2], "\n",
    "Korrelation mellem størrelse og besøgende (uden", rownames(tbl)[unusual], "):", 
    correlation_without_outlier[1,2], "\n\n")
```

Kommentar til outliers:
Scatterplottet viser, at Great Smoky er en tydelig outlier med markant flere besøgende i forhold til parkens størrelse. Når vi beregner korrelationen med og uden denne outlier, ser vi en ændring fra 0.173 til 0.391 . Dette indikerer, at denne park har en betydelig indflydelse på den lineære sammenhæng mellem parkstørrelse og antal besøgende.

## Opgave 4.40.b

Bestem poweroverførslen $\lambda_1$, der gør $x_1$-værdierne tilnærmelsesvis normale. Konstruer et Q-Q plot af de transformerede observationer.

*Hint: Brug Box-Cox transformation til at finde den optimale $\lambda$-værdi.*

```{r}
# Konverter dataframe til matrix for at bruge funktioner fra R-scriptet
tbl <- as.matrix(tbl)

# Box-Cox transformation for X1 (Size)
tst <- boxcox(tbl[, 1] ~ 1)
lambda1 <- tst$x[which(tst$y == max(tst$y))]
cat("Den optimale lambda-værdi for X1 (Size) er:", lambda1, "\n")

# Verificer med powerTransform
pt_result1 <- powerTransform(tbl[,1])
print(pt_result1)

# Transformer værdierne
X1_transformed <- (tbl[,1]^lambda1 - 1) / lambda1

# Plot 2 plots i én figur
par(mfrow=c(1,2))

# QQ Plot af transformerede værdier
qqnorm(X1_transformed, main = "Normal Q-Q Plot af transformeret X1 (Size)")
qqline(X1_transformed)

# QQ plot af originale værdier
qqnorm(tbl[,1], main = "Normal Q-Q Plot af X1 (Size)")
qqline(tbl[,1])
```
## Opgave 4.40.c

Bestem poweroverførslen $\lambda_2$, der gør $x_2$-værdierne tilnærmelsesvis normale. Konstruer et Q-Q plot af de transformerede observationer.

```{r}
# Box-Cox transformation for X2 (Visitors)
tst <- boxcox(tbl[, 2] ~ 1)
lambda2 <- tst$x[which(tst$y == max(tst$y))]
cat("Den optimale lambda-værdi for X2 (Visitors) er:", lambda2, "\n")

# Verificer med powerTransform
pt_result2 <- powerTransform(tbl[,2])
print(pt_result2)

# Transformer værdierne
X2_transformed <- (tbl[,2]^lambda2 - 1) / lambda2

# Plot 2 plots i én figur
par(mfrow=c(1,2))

# QQ Plot af transformerede værdier
qqnorm(X2_transformed, main = "Normal Q-Q Plot af transformeret X2 (Visitors)")
qqline(X2_transformed)

# QQ plot af originale værdier
qqnorm(tbl[,2], main = "Normal Q-Q Plot af X2 (Visitors)")
qqline(tbl[,2])
```

## Opgave 4.40.d

Bestem poweroverførslen for tilnærmelsesvis bivariat normalitet ved brug af (4-40).

*Hint: Formlen (4-40) giver et vægtet gennemsnit af de individuelle transformationsparametre.*

```{r}
powerTransform(tbl)
```
