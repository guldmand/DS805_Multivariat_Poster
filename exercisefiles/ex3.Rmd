---
title: "Kapitel 2: Matrixalgebra og Stokastiske Vektorer - Øvelser"
author: "Anvendt Multivariat Statistisk Analyse"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: false
    theme: flatly
    highlight: tango
    df_print: paged
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  comment = NA,
  fig.align = "center",
  out.width = "80%"
)
```

# Øvelse 2.3

Verificer følgende egenskaber for transponeringen når
$$
\mathbf{A} = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}, \quad
\mathbf{B} = \begin{bmatrix} 1 & 4 & 2 \\ 5 & 0 & 3 \end{bmatrix}, \quad
\mathbf{C} = \begin{bmatrix} 1 & 4 \\ 3 & 2 \end{bmatrix}
$$

## Del (a)
Verificer at $(\mathbf{A}')' = \mathbf{A}$

```{r}
# Definér matrix A
A <- matrix(c(2, 1, 1, 3), 2, 2)

# Print matricen
print(A)

# Transponer A
A_t <- t(A)
print(A_t)

# Transponer den transponerede matrix
A_tt <- t(A_t)
print(A_tt)

# Verificer at de er ens
all.equal(A, A_tt)
```

## Del (b)
Verificer at $(\mathbf{C}')^{-1} = (\mathbf{C}^{-1})'$

```{r}
# Definér matrix C
C <- matrix(c(1, 3, 4, 2), 2, 2)

# Beregn (C')^{-1}
venstre_side <- solve(t(C))
print(venstre_side)

# Beregn (C^{-1})'
højre_side <- t(solve(C))
print(højre_side)

# Verificer egenskaben
all.equal(venstre_side, højre_side)
```

Vi har verificeret egenskaben $(\mathbf{C}')^{-1} = (\mathbf{C}^{-1})'$, som viser at den inverse af den transponerede matrix er lig med den transponerede af den inverse matrix.

## Del (c)
Verificer at $(\mathbf{AB})' = \mathbf{B}'\mathbf{A}'$

```{r}
# Definér matricer A og B
A <- matrix(c(2, 1, 1, 3), 2, 2)
B <- matrix(c(1, 5, 4, 0, 2, 3), 2, 3)

# Beregn A*B
AB <- A %*% B
print(AB)

# Beregn (A*B)'
venstre_side <- t(AB)
print(venstre_side)

# Beregn B'A'
højre_side <- t(B) %*% t(A)
print(højre_side)

# Verificer egenskaben
all.equal(venstre_side, højre_side)
```

Vi har verificeret egenskaben $(\mathbf{AB})' = \mathbf{B}'\mathbf{A}'$, som viser at transponeringen af et matrixprodukt er lig med produktet af de transponerede matricer i omvendt rækkefølge.


# Øvelse 2.5

Kontrollér at
$$\mathbf{Q} = \begin{bmatrix} \frac{5}{13} & \frac{12}{13} \\ -\frac{12}{13} & \frac{5}{13} \end{bmatrix}$$
er en ortogonal matrix.

En ortogonal matrix $\mathbf{Q}$ opfylder betingelsen $\mathbf{Q}'\mathbf{Q} = \mathbf{I}$, hvor $\mathbf{I}$ er identitetsmatricen.

```{r}
# Definér matrix Q
Q <- matrix(c(5, -12, 12, 5) / 13, 2, 2)

# Vis matrix Q
cat("Matrix Q:\n")

# Beregn Q*Q'
QQt <- Q %*% t(Q)
print(round(QQt, digits = 2))

# Beregn Q'*Q
QtQ <- t(Q) %*% Q
print(round(QtQ, digits = 2))

# Verificer at Q er ortogonal
cat("Test Q*Q' = I: ", all.equal(QQt, diag(2)), "\n")
cat("Test Q'*Q = I: ", all.equal(QtQ, diag(2)), "\n")
```

# Øvelse 2.6

Lad
$$\mathbf{A} = \begin{bmatrix} 9 & -2 \\ -2 & 6 \end{bmatrix}$$

## Del (a)
Er $\mathbf{A}$ symmetrisk?

```{r}
# Definér matrix A
A <- matrix(c(9, -2, -2, 6), 2, 2)

# Vis matrix A
print(A)

# Beregn A'
A_t <- t(A)
print(A_t)

# Verificer om A er symmetrisk
cat("A = A'? ", all.equal(A, A_t), "\n")
```

En matrix er symmetrisk hvis $\mathbf{A} = \mathbf{A}'$. Vi har verificeret at $\mathbf{A} = \mathbf{A}'$, så matrix $\mathbf{A}$ er symmetrisk.

## Del (b): 
Vis at $\mathbf{A}$ er positiv definit

```{r}
# Metode 1: Tjek determinanten og diagonalelementerne
diag(A)
det(A)

# Metode 2: Tjek egenværdierne
egen_A <- eigen(A)
print(egen_A$values)

# Verificer at A er positiv definit
all(egen_A$values > 0)
```

En symmetrisk matrix er positiv definit hvis alle dens egenværdier er positive. Vi har verificeret at begge egenværdier (10 og 5) er positive. Alternativt kan vi også bekræfte at matrix $\mathbf{A}$ er positiv definit fordi:
1. Alle diagonalelementer er positive (9 og 6)
2. Determinanten er positiv (9 × 6 - (-2) × (-2) = 54 - 4 = 50 > 0)

# Øvelse 2.7

Lad $\mathbf{A}$ være som i Øvelse 2.6.

## Del (a)
Bestem egenværdierne og egenvektorerne for $\mathbf{A}$

```{r}
# Definér matrix A
A <- matrix(c(9, -2, -2, 6), 2, 2)

# Beregn egenværdier og egenvektorer
ea <- eigen(A)

# Vis egenværdierne
print(ea$values)

# Vis egenvektorerne
print(ea$vectors)
```

Matrix $\mathbf{A}$ har egenværdierne $\lambda_1 = 10$ og $\lambda_2 = 5$ med tilhørende egenvektorer som vist i tabellen. Hver egenvektor $\mathbf{v}_i$ opfylder ligningen $\mathbf{A}\mathbf{v}_i = \lambda_i\mathbf{v}_i$.

## Del (b): 
Skriv den spektrale dekomposition af $\mathbf{A}$

```{r}
# Beregn spektral dekomposition
L <- ea$values
V <- ea$vectors

# Metode 1: PDP'
P <- V
D <- diag(L)
PDP_t <- P %*% D %*% t(P)

# Metode 2: Sum af λᵢvᵢvᵢ'
spektral_dekomp <- L[1] * V[,1] %*% t(V[,1]) + L[2] * V[,2] %*% t(V[,2])

# Vis original matrix
print(A)

# Vis spektral dekomposition (Metode 1)
print(round(PDP_t, digits = 2))

# Vis spektral dekomposition (Metode 2)
print(round(spektral_dekomp, digits = 2))

# Verificer at begge metoder giver samme resultat
all.equal(PDP_t, spektral_dekomp)
```

Den spektrale dekomposition af en symmetrisk matrix $\mathbf{A}$ er givet ved $\mathbf{A} = \mathbf{P}\mathbf{D}\mathbf{P}'$, hvor $\mathbf{P}$ er en matrix med egenvektorer som kolonner, og $\mathbf{D}$ er en diagonalmatrix med egenværdier på diagonalen. Alternativt kan den skrives som $\mathbf{A} = \sum_{i=1}^{n} \lambda_i \mathbf{v}_i \mathbf{v}_i'$. Vi har verificeret begge formuleringer og bekræftet at de giver samme resultat.

## Del (c)
Find $\mathbf{A}^{-1}$

```{r}
# Beregn den inverse matrix
A_inv <- solve(A)

# Vis den inverse matrix
print(A_inv)

# Verificer at A*A^{-1} = I
I_check <- A %*% A_inv
print(round(I_check, digits = 10))
all.equal(I_check, diag(2))
```

Vi har beregnet $\mathbf{A}^{-1}$ og verificeret at $\mathbf{A}\mathbf{A}^{-1} = \mathbf{I}$, hvilket bekræfter at vi har fundet den korrekte inverse matrix.

## Del (d)
Find egenværdierne og egenvektorerne for $\mathbf{A}^{-1}$

```{r}
# Beregn egenværdier og egenvektorer for A^{-1}
ea_inv <- eigen(A_inv)

# Vis data i en tabel
sammenligning <- data.frame(
  "Egenværdier af A" = ea$values,
  "Egenværdier af A^{-1}" = ea_inv$values,
  "1/Egenværdier af A" = 1/ea$values
)

# Vis sammenligningen 
print(sammenligning)

# Vis egenvektorer
print(ea_inv$vectors)
```

Vi har observeret at:
1. Egenværdierne af $\mathbf{A}^{-1}$ er de reciprokke af egenværdierne af $\mathbf{A}$, dvs. $\lambda_{A^{-1}} = 1/\lambda_A$.
2. Egenvektorerne af $\mathbf{A}^{-1}$ er de samme som egenvektorerne af $\mathbf{A}$ (op til et fortegn).

Dette bekræfter den teoretiske sammenhæng mellem egenværdier og egenvektorer for en matrix og dens inverse.

# Supplerende undersøgelser relateret til 2.6 og 2.7

```{r}
# Definér matrix A og find dens egenvektorer
A <- matrix(c(9, -2, -2, 6), 2, 2)
P <- eigen(A)$vectors

# Undersøg om egenvektormatricen P er ortogonal
PtP <- t(P) %*% P
print(round(PtP, digits = 2))

PPt <- P %*% t(P)
# Tjek om P*P' = I
print(round(PPt, digits = 2))

# Tjek andre egenskaber
D <- diag(eigen(A)$values)
Ainv_check <- P %*% solve(D) %*% t(P)

# Verifikation af A^{-1} = P*D^{-1}*P'
print(round(Ainv_check, digits = 2))
print(round(solve(A), digits = 2))

# Bekræft at A^{-1} = P*D^{-1}*P'
all.equal(Ainv_check, solve(A))

# Tjek spor og determinant egenskaber
egenskaber <- data.frame(
  Egenskab = c("Sum af egenværdier", "Trace af A", 
              "Produkt af egenværdier", "Determinant af A"),
  Værdi = c(sum(eigen(A)$values), sum(diag(A)), 
           prod(eigen(A)$values), det(A))
)

# Print egenskaberne
print(egenskaber)
```

1. Vi har verificeret at egenvektormatricen $\mathbf{P}$ er ortogonal, da $\mathbf{P}'\mathbf{P} \approx \mathbf{I}$ og $\mathbf{P}\mathbf{P}' \approx \mathbf{I}$.
2. Vi har bekræftet at $\mathbf{A}^{-1} = \mathbf{P}\mathbf{D}^{-1}\mathbf{P}'$, hvor $\mathbf{D}$ er diagonalmatricen med egenværdier.
3. Vi har verificeret at summen af egenværdierne er lig med sporet af matricen: $\sum_i \lambda_i = \text{tr}(\mathbf{A})$.
4. Vi har verificeret at produktet af egenværdierne er lig med determinanten af matricen: $\prod_i \lambda_i = \det(\mathbf{A})$.

# Øvelse 2.24

Lad $\mathbf{X}$ have kovariansmatrix
$$\boldsymbol{\Sigma} = \begin{bmatrix} 
4 & 0 & 0 \\ 
0 & 9 & 0 \\ 
0 & 0 & 1
\end{bmatrix}$$

## Del (a)
Find $\boldsymbol{\Sigma}^{-1}$

```{r}
# Definér kovariansmatricen
Sigma <- diag(c(4, 9, 1))
print(Sigma)

# Beregn Sigma^{-1}
Sigma_inv <- solve(Sigma)
print(Sigma_inv)

# Alternativ metode: reciprokke af diagonalelementer
Sigma_inv_alt <- diag(1/diag(Sigma))
print(Sigma_inv_alt)

# Verificer at de to metoder giver samme resultat
all.equal(Sigma_inv, Sigma_inv_alt)
```

For en diagonalmatrix $\mathbf{D}$ er den inverse matrix $\mathbf{D}^{-1}$ også en diagonalmatrix, hvor hvert diagonalelement er det reciprokke af det tilsvarende element i $\mathbf{D}$. Vi har bekræftet dette ved at beregne $\boldsymbol{\Sigma}^{-1}$ både med standard matrixinversion og ved at tage de reciprokke af diagonalelementerne.

## Del (b)
Find egenværdierne og egenvektorerne for $\boldsymbol{\Sigma}$

```{r}
# Beregn egenværdier og egenvektorer for Sigma
eigen_Sigma <- eigen(Sigma)

# Vis egenværdierne
print(eigen_Sigma$values)

# Vis egenvektorerne
print(eigen_Sigma$vectors)

# Verificer at egenværdierne er lig med diagonalelementerne
diag(Sigma)
eigen_Sigma$values
```

For en diagonalmatrix er egenværdierne lig med diagonalelementerne, og egenvektorerne er standardbasisvektorerne (vektorer der har 1 på én position og 0 på alle andre). Vi har bekræftet at egenværdierne af $\boldsymbol{\Sigma}$ er 9, 4 og 1, og at egenvektorerne er standardbasisvektorer (i dette eksempel i en anden rækkefølge).

## Del (c)
Find egenværdierne og egenvektorerne for $\boldsymbol{\Sigma}^{-1}$

```{r}
# Beregn egenværdier og egenvektorer for Sigma^{-1}
eigen_Sigma_inv <- eigen(Sigma_inv)

# Vis egenværdierne
print(eigen_Sigma_inv$values)

# Vis egenvektorerne
print(eigen_Sigma_inv$vectors)

# Sammenligning af egenværdier
sammenligning <- data.frame(
  "Egenværdier af Sigma" = sort(eigen_Sigma$values, decreasing = TRUE),
  "Egenværdier af Sigma^{-1}" = sort(eigen_Sigma_inv$values, decreasing = TRUE),
  "1/Egenværdier af Sigma" = 1/sort(eigen_Sigma$values, decreasing = TRUE)
)

# Print sammenligningen
print(sammenligning)
```

 Vi observerer at:
1. Egenværdierne af $\boldsymbol{\Sigma}^{-1}$ er de reciprokke af egenværdierne af $\boldsymbol{\Sigma}$, dvs. $\lambda_{\Sigma^{-1}} = 1/\lambda_\Sigma$.
2. Egenvektorerne af $\boldsymbol{\Sigma}^{-1}$ er de samme som egenvektorerne af $\boldsymbol{\Sigma}$ (selvom rækkefølgen kan variere).

Dette bekræfter det generelle resultat, at egenværdierne af den inverse matrix er de reciprokke af den oprindelige matrix' egenværdier, mens egenvektorerne forbliver de samme.